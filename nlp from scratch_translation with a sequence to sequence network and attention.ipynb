{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token=0\n",
    "EOS_token=1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self,name):\n",
    "        self.name=name\n",
    "        self.word2index={}\n",
    "        self.word2count={}\n",
    "        self.index2word={0:\"SOS\",1:\"EOS\"}\n",
    "        self.n_words=2\n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word]=self.n_words\n",
    "            self.word2count[word]=1\n",
    "            self.index2word[self.n_words]=word\n",
    "            self.n_words+=1\n",
    "        else:\n",
    "            self.word2count[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='Mn')\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1,lang2,reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    lines=open(\"data/data/%s-%s.txt\" % (lang1,lang2),encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs=[[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs=[list(reversed(p)) for p in pairs]\n",
    "        input_lang=Lang(lang2)\n",
    "        output_lang=Lang(lang1)\n",
    "    else:\n",
    "        input_lang=Lang(lang1)\n",
    "        output_lang=Lang(lang2)\n",
    "    return input_lang,output_lang,pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' '))<MAX_LENGTH and len(p[1].split(' '))<MAX_LENGTH and p[1].startswith(eng_prefixes)\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words\n",
      "fra 4345\n",
      "eng 2803\n",
      "['elle est en train de cuisiner pour lui .', 'she is cooking for him .']\n"
     ]
    }
   ],
   "source": [
    "def prepareDate(lang1,lang2,reverse=False):\n",
    "    input_lang,output_lang,pairs=readLangs(lang1,lang2,reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs=filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words\")\n",
    "    print(input_lang.name,input_lang.n_words)\n",
    "    print(output_lang.name,output_lang.n_words)\n",
    "    return input_lang,output_lang,pairs\n",
    "\n",
    "input_lang,output_lang,pairs=prepareDate('eng','fra',True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding=nn.Embedding(input_size,hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size)\n",
    "    def forward(self,input,hidden):\n",
    "        embedded=self.embedding(input).view(1,1,-1)\n",
    "        output=embedded\n",
    "        output,hidden=self.gru(output,hidden)\n",
    "        return output,hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,output_size):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding=nn.Embedding(output_size,hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size)\n",
    "        self.out=nn.Linear(hidden_size,output_size)\n",
    "        self.softmax=nn.LogSoftmax(dim=1)\n",
    "    def forward(self,input,hidden):\n",
    "        output=self.embedding(input).view(1,1,-1)\n",
    "        output=F.relu(output)\n",
    "        output,hidden=self.gru(output,hidden)\n",
    "        output=self.softmax(self.out(output[0]))\n",
    "        return output,hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self,hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100]) torch.Size([1, 1, 128]) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,output_size,dropout_p=0.1,max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.output_size=output_size\n",
    "        self.dropout_p=dropout_p\n",
    "        self.max_length=max_length\n",
    "        \n",
    "        self.embedding=nn.Embedding(self.output_size,self.hidden_size)\n",
    "        self.attn=nn.Linear(self.hidden_size*2,self.max_length)\n",
    "        self.attn_combine=nn.Linear(self.hidden_size*2,self.hidden_size)\n",
    "        self.dropout=nn.Dropout(self.dropout_p)\n",
    "        self.gru=nn.GRU(self.hidden_size,self.hidden_size)\n",
    "        self.out=nn.Linear(self.hidden_size,self.output_size)\n",
    "    def forward(self,input,hidden,encoder_outputs):\n",
    "        embedded=self.embedding(input).view(1,1,-1)   #[1,1,128]\n",
    "        embedded=self.dropout(embedded)\n",
    "        temp=torch.cat((embedded[0],hidden[0]),1)\n",
    "        #print(\"temp:\",temp.size())\n",
    "        attn_weights=F.softmax(self.attn(temp),dim=1)\n",
    "        #print(\"attn_weight:\",attn_weight.size())\n",
    "        #print(\"encoder_outputs:\",encoder_outputs.size())\n",
    "        attn_applied=torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "        #print(\"attn_applied:\",attn_applied.size())\n",
    "        output=torch.cat((embedded[0],attn_applied[0]),1)\n",
    "        output=self.attn_combine(output).unsqueeze(0)    #[1,1,128]\n",
    "        output=F.relu(output)\n",
    "        output,hidden=self.gru(output,hidden)\n",
    "        output=F.log_softmax(self.out(output[0]),dim=1)\n",
    "        return output,hidden,attn_weights\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size,device=device)\n",
    "\n",
    "net=AttnDecoderRNN(128,100,0.1,10).to(device)\n",
    "\n",
    "encoder_outputs=torch.zeros(10,128,device=device)\n",
    "\n",
    "decoder_input=torch.tensor([[0]],device=device)\n",
    "\n",
    "output,hidden,attn_weight=net(decoder_input,torch.zeros(1,1,128).to(device),encoder_outputs)\n",
    "\n",
    "print(output.size(),hidden.size(),attn_weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang,sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "def tensorFromSentencec(lang,sentence):\n",
    "    indexes=indexesFromSentence(lang,sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes,dtype=torch.long,device=device).view(-1,1)\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor=tensorFromSentencec(input_lang,pair[0])\n",
    "    target_tensor=tensorFromSentencec(output_lang,pair[1])\n",
    "    return (input_tensor,target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio=0.5\n",
    "def train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion,max_length=MAX_LENGTH):\n",
    "    encoder_hidden=encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length=input_tensor.size(0)\n",
    "    target_length=target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs=torch.zeros(max_length,encoder.hidden_size,device=device)\n",
    "    loss=0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output,encoder_hidden=encoder(input_tensor[ei],encoder_hidden)\n",
    "        encoder_outputs[ei]=encoder_output[0,0]\n",
    "    decoder_input=torch.tensor([[SOS_token]],device=device)\n",
    "    decoder_hidden=encoder_hidden\n",
    "    use_teacher_forcing=True if random.random()<teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            #print(decoder_input.size(),decoder_hidden.size(),encoder_outputs.size())\n",
    "            decoder_output,decoder_hidden,decoder_attention=decoder(\n",
    "                            decoder_input,decoder_hidden,encoder_outputs)\n",
    "            \n",
    "            loss+=criterion(decoder_output,target_tensor[di])\n",
    "            decoder_input=target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output,decoder_hidden,decoder_attention=decoder(\n",
    "                        decoder_input,decoder_hidden,encoder_outputs\n",
    "            )\n",
    "            topv,topi=decoder_output.topk(1)\n",
    "            decoder_input=topi.squeeze().detach()\n",
    "            loss+=criterion(decoder_output,target_tensor[di])\n",
    "            if decoder_input.item()==EOS_token:\n",
    "                break\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item()/target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import math\n",
    "def asMinutes(s):\n",
    "    m=math.floor(s/60)\n",
    "    s-=m*60\n",
    "    return '%dm %ds'%(m,s)\n",
    "def timeSince(since,percent):\n",
    "    now=time.time()\n",
    "    s=now-since\n",
    "    es=s/(percent)\n",
    "    rs=es-s\n",
    "    return '%s (- %s)' %(asMinutes(s),asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder,decoder,n_iters,print_every=1000,plot_every=100,learning_rate=0.01):\n",
    "    start=time.time()\n",
    "    plot_losses=[]\n",
    "    print_loss_total=0\n",
    "    plot_loss_total=0\n",
    "    \n",
    "    encoder_optimizer=optim.SGD(encoder.parameters(),lr=learning_rate)\n",
    "    decoder_optimizer=optim.SGD(decoder.parameters(),lr=learning_rate)\n",
    "    #[([2,5,6,1],[5,9,8,1]),(),()...]\n",
    "    training_pairs=[tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion=nn.NLLLoss()\n",
    "    for iter in range(1,n_iters+1):\n",
    "        training_pair=training_pairs[iter-1]\n",
    "        input_tensor=training_pair[0]\n",
    "        target_tensor=training_pair[1]\n",
    "        \n",
    "        loss=train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion)\n",
    "        print_loss_total+=loss\n",
    "        plot_loss_total+=loss\n",
    "        if iter % print_every==0:\n",
    "            print_loss_avg=print_loss_total/print_every\n",
    "            print_loss_total=0\n",
    "            print(\"%s (%d %d%%)  %.4f\"%(timeSince(start,iter/n_iters),iter,iter/n_iters*100,print_loss_avg))\n",
    "        if iter%plot_every==0:\n",
    "            plot_loss_avg=plot_loss_total/plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total=0\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder,decoder,sentence,max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor=tensorFromSentencec(input_lang,sentence)\n",
    "        input_length=input_tensor.size(0)\n",
    "        encoder_hidden=encoder.initHidden()\n",
    "        encoder_outputs=torch.zeros(max_length,encoder.hidden_size,device=device)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output,encoder_hidden=encoder(input_tensor[ei],encoder_hidden)\n",
    "            encoder_outputs[ei]+=encoder_output[0,0]\n",
    "        decoder_input=torch.tensor([[SOS_token]],device=device)\n",
    "        decoder_hidden=encoder_hidden\n",
    "        decoded_words=[]\n",
    "        decoder_attentions=torch.zeros(max_length,max_length)\n",
    "        for di in range(max_length):\n",
    "            decoder_output,decoder_hidden,decoder_attention=decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "            decoder_attentions[di]=decoder_attention.data\n",
    "            topv,topi=decoder_output.topk(1)\n",
    "            if topi.item()==EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "                decoder_input=topi.squeeze().detach()\n",
    "        return decoded_words,decoder_attentions[:di+1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder,decoder,n=10):\n",
    "    for i in range(n):\n",
    "        pair=random.choice(pairs)\n",
    "        print('>',pair[0])\n",
    "        print('=',pair[1])\n",
    "        output_words,attentions=evaluate(encoder,decoder,pair[0])\n",
    "        output_sentence=' '.join(output_words)\n",
    "        print('<',output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 53s (- 82m 22s) (5000 6%)  2.8586\n",
      "11m 40s (- 75m 50s) (10000 13%)  2.2760\n",
      "17m 32s (- 70m 8s) (15000 20%)  1.9898\n",
      "23m 21s (- 64m 13s) (20000 26%)  1.6991\n",
      "29m 7s (- 58m 15s) (25000 33%)  1.5275\n",
      "34m 54s (- 52m 21s) (30000 40%)  1.3616\n",
      "40m 40s (- 46m 29s) (35000 46%)  1.2105\n",
      "46m 30s (- 40m 41s) (40000 53%)  1.0932\n",
      "52m 20s (- 34m 53s) (45000 60%)  1.0012\n",
      "58m 0s (- 29m 0s) (50000 66%)  0.8894\n",
      "63m 54s (- 23m 14s) (55000 73%)  0.8031\n",
      "70m 3s (- 17m 30s) (60000 80%)  0.7684\n",
      "76m 24s (- 11m 45s) (65000 86%)  0.7362\n",
      "82m 59s (- 5m 55s) (70000 93%)  0.6385\n",
      "90m 2s (- 0m 0s) (75000 100%)  0.6056\n"
     ]
    }
   ],
   "source": [
    "hidden_size=256\n",
    "encoder1=EncoderRNN(input_lang.n_words,hidden_size).to(device)\n",
    "attn_decoder1=AttnDecoderRNN(hidden_size,output_lang.n_words,dropout_p=0.1).to(device)\n",
    "trainIters(encoder1,attn_decoder1,75000,print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je ne suis pas votre ennemi .\n",
      "= i m not your enemy .\n",
      "< i am not your enemy . <EOS>\n",
      "\n",
      "> il ramasse l argent a la pelle .\n",
      "= he s raking it in .\n",
      "< he s in in the . <EOS>\n",
      "\n",
      "> il a offert son assistance .\n",
      "= he s offered to help .\n",
      "< he is used to us . <EOS>\n",
      "\n",
      "> je suis impatient de vous voir .\n",
      "= i m anxious to see you .\n",
      "< i m looking forward to seeing you . <EOS>\n",
      "\n",
      "> j ai peur d avoir une hemorragie interne .\n",
      "= i m afraid i have internal bleeding .\n",
      "< i m afraid i have internal bleeding . <EOS>\n",
      "\n",
      "> elle est maladroite .\n",
      "= she is awkward .\n",
      "< she is forgetful . <EOS>\n",
      "\n",
      "> vous etes charmants .\n",
      "= you re charming .\n",
      "< you re rude . <EOS>\n",
      "\n",
      "> je suis impatiente de te voir danser .\n",
      "= i m looking forward to seeing you dance .\n",
      "< i m looking forward to seeing you . <EOS>\n",
      "\n",
      "> il est grand et maigre .\n",
      "= he is tall and lean .\n",
      "< he is tall and ruthless . <EOS>\n",
      "\n",
      "> je suis tellement perplexe .\n",
      "= i m so confused .\n",
      "< i m so confused . <EOS>\n",
      "\n",
      "input = elle a cinq ans de moins que moi .\n",
      "output = she s got years younger than i am . <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = elle est trop petit .\n",
      "output = she s an to him . <EOS>\n",
      "input = je ne crains pas de mourir .\n",
      "output = i m not scared to die . <EOS>\n",
      "input = c est un jeune directeur plein de talent .\n",
      "output = he s a talented talented . <EOS>\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1,attn_decoder1)\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x175931485f8>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
    "plt.matshow(attentions.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = elle a cinq ans de moins que moi .\n",
      "output = she is five years younger than i am . <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = elle est trop petit .\n",
      "output = she s an to him . <EOS>\n",
      "input = je ne crains pas de mourir .\n",
      "output = i m not scared to die . <EOS>\n",
      "input = c est un jeune directeur plein de talent .\n",
      "output = he s a talented young . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
